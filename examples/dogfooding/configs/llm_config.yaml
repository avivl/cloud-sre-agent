# Dogfooding LLM Configuration
# This configuration is optimized for the dogfooding environment with local Ollama models

# Main configuration
default_provider: "ollama"
default_model_type: "fast"
enable_fallback: true
enable_monitoring: true

# Provider configurations
providers:
  # Ollama Provider (Primary for dogfooding)
  ollama:
    provider: "ollama"
    base_url: "http://localhost:11434"
    timeout: 120
    max_retries: 2
    rate_limit: null
    models:
      llama3.1:70b:
        name: "llama3.1:70b"
        model_type: "smart"
        cost_per_1k_tokens: 0.0
        max_tokens: 4000
        supports_streaming: true
        supports_tools: false
        capabilities: ["reasoning", "code_generation", "analysis", "structured_output"]
        performance_score: 0.6
        reliability_score: 0.95
        quality_score: 0.95
      llama3.1:8b:
        name: "llama3.1:8b"
        model_type: "fast"
        cost_per_1k_tokens: 0.0
        max_tokens: 4000
        supports_streaming: true
        supports_tools: false
        capabilities: ["reasoning", "code_generation", "analysis"]
        performance_score: 0.8
        reliability_score: 0.8
      codellama:13b:
        name: "codellama:13b"
        model_type: "fast"
        cost_per_1k_tokens: 0.0
        max_tokens: 4000
        supports_streaming: true
        supports_tools: false
        capabilities: ["reasoning", "code_generation"]
        performance_score: 0.8
        reliability_score: 0.8
    provider_specific:
      host: "localhost"
      port: 11434
      tls: false

# Agent configurations (Dogfooding-optimized)
agents:
  analysis_agent:
    primary_provider: "ollama"
    fallback_providers: []
    model_selection_strategy: "speed_optimized"
    max_tokens: 8000
    temperature: 0.3
    timeout: 60
    retry_attempts: 2
    enable_streaming: true
    enable_tools: false
    model_preferences:
      - "llama3.1:70b"
      - "llama3.1:8b"
      - "codellama:13b"

  triage_agent:
    primary_provider: "ollama"
    fallback_providers: []
    model_selection_strategy: "speed_optimized"
    max_tokens: 4000
    temperature: 0.1
    timeout: 30
    retry_attempts: 1
    enable_streaming: false
    enable_tools: false
    model_preferences:
      - "llama3.1:70b"
      - "llama3.1:8b"
      - "codellama:13b"

  remediation_agent:
    primary_provider: "ollama"
    fallback_providers: []
    model_selection_strategy: "quality_optimized"
    max_tokens: 16000
    temperature: 0.2
    timeout: 90
    retry_attempts: 2
    enable_streaming: true
    enable_tools: false
    model_preferences:
      - "llama3.1:70b"
      - "llama3.1:8b"
      - "codellama:13b"

# Cost management configuration (Dogfooding-friendly)
cost_config:
  monthly_budget: 1.0 # Small budget for local models
  alert_threshold: 0.8
  enable_tracking: true
  cost_optimization: false
  optimization_strategy: "performance"
  cost_breakdown:
    by_provider: true
    by_model: true
    by_agent: true
  alerts:
    email: "dev@company.com"
    webhook: null
    threshold_percentages: [50, 75, 90, 100]
  daily_limit: 0.1
  weekly_limit: 0.5

# Resilience configuration (Dogfooding-optimized)
resilience_config:
  circuit_breaker_threshold: 3
  circuit_breaker_timeout: 30
  retry_attempts: 2
  retry_delay: 0.5
  retry_backoff: 1.5
  fallback_enabled: false
  health_check_interval: 30
  timeout_multiplier: 1.2
  jitter_enabled: true
  max_concurrent_requests: 5
  rate_limiting:
    enabled: false
    requests_per_minute: 1000
    burst_size: 10
