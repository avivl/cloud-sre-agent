# gemini_sre_agent/core/types/__init__.py

"""
Core type definitions for the Gemini SRE Agent system.

This package provides comprehensive type aliases, protocols, and utilities
for type-safe development across all components.
"""

from .agent import (
    AgentCapabilities,
    AgentContext,
    AgentId,
    AgentMetadata,
    AgentName,
    AgentRequest,
    AgentResponse,
    AgentStatus,
    AgentT,
    AnalysisAgent,
    AnalysisResponse,
    BaseAgent,
    ConfigurableAgent,
    CoordinationChannel,
    CoordinationEvent,
    CoordinationMessage,
    LoggableAgent,
    OperationStatus,
    OperationType,
    PromptContext,
    PromptTemplate,
    PromptVariables,
    RemediationAgent,
    RemediationResponse,
    RequestT,
    RequestType,
    ResponseConfidence,
    ResponseContent,
    ResponseMetadata,
    ResponseT,
    ResponseType,
    StatefulAgent,
    StateKey,
    StateSnapshot,
    StateValue,
    TriageAgent,
    TriageResponse,
    WorkflowId,
    WorkflowStatus,
    WorkflowStep,
    create_agent_context,
    validate_agent_request,
    validate_agent_response,
)
from .base import (
    ConfigDict,
    Configurable,
    ConfigValue,
    Content,
    ContentEncoding,
    ContentType,
    Deserializable,
    Duration,
    ErrorCode,
    ErrorDetails,
    ErrorMessage,
    Identifiable,
    JsonDict,
    JsonList,
    JsonValue,
    LogContext,
    Loggable,
    LogLevel,
    LogMessage,
    MetricName,
    MetricTags,
    MetricTimestamp,
    MetricValue,
    Priority,
    ProcessingStatus,
    RequestId,
    ResponseId,
    Serializable,
    SessionId,
    Stateful,
    Timeout,
    Timestamp,
    Timestamped,
    UserId,
    Validatable,
    create_type_safe_dict,
    ensure_json_value,
    is_json_value,
)
from .llm import (
    ChatProvider,
    ChatRequest,
    ChatResponse,
    CompletionProvider,
    CompletionRequest,
    CompletionResponse,
    ContentFormat,
    CostBreakdown,
    CostPerToken,
    EmbeddingProvider,
    EmbeddingRequest,
    EmbeddingResponse,
    ErrorType,
    Latency,
    LLMConfig,
    LLMModel,
    LLMProvider,
    LLMRequest,
    LLMResponse,
    Message,
    ModelCapability,
    ModelConfig,
    ModelId,
    ModelStatus,
    ModelT,
    ModelType,
    ProviderConfig,
    ProviderId,
    ProviderStatus,
    ProviderT,
    ProviderType,
    RateLimit,
    RequestRole,
    ResponseRole,
    RetryAfter,
    StreamingChunk,
    StreamingResponse,
    Throughput,
    TokenCount,
    TokenUsage,
    TotalCost,
    calculate_token_cost,
    create_message,
    create_usage_dict,
    validate_model_config,
)
from .llm import ContentType as LLMContentType
from .llm import ErrorCode as LLMErrorCode
from .llm import ResponseT as LLMResponseT

__all__ = [
    # Base types
    "JsonValue",
    "JsonDict",
    "JsonList",
    "ConfigDict",
    "ConfigValue",
    "Content",
    "ContentType",
    "ContentEncoding",
    "LogLevel",
    "LogMessage",
    "LogContext",
    "AgentId",
    "AgentName",
    "AgentStatus",
    "ModelId",
    "ProviderId",
    "RequestId",
    "ResponseId",
    "SessionId",
    "UserId",
    "Priority",
    "ProcessingStatus",
    "Timestamp",
    "Duration",
    "Timeout",
    "ErrorCode",
    "ErrorMessage",
    "ErrorDetails",
    "MetricName",
    "MetricValue",
    "MetricTags",
    "MetricTimestamp",
    # Base protocols
    "Serializable",
    "Deserializable",
    "Identifiable",
    "Timestamped",
    "Configurable",
    "Stateful",
    "Loggable",
    "Validatable",
    # Base utilities
    "is_json_value",
    "ensure_json_value",
    "create_type_safe_dict",
    # Agent types
    "AgentT",
    "RequestT",
    "ResponseT",
    "AgentContext",
    "AgentMetadata",
    "AgentCapabilities",
    "RequestType",
    "ResponseType",
    "OperationType",
    "OperationStatus",
    "PromptTemplate",
    "PromptVariables",
    "PromptContext",
    "ResponseContent",
    "ResponseMetadata",
    "ResponseConfidence",
    "StateKey",
    "StateValue",
    "StateSnapshot",
    "CoordinationMessage",
    "CoordinationChannel",
    "CoordinationEvent",
    "WorkflowId",
    "WorkflowStep",
    "WorkflowStatus",
    # Agent protocols
    "BaseAgent",
    "AgentRequest",
    "AgentResponse",
    "StatefulAgent",
    "ConfigurableAgent",
    "LoggableAgent",
    "TriageAgent",
    "AnalysisAgent",
    "RemediationAgent",
    "TriageResponse",
    "AnalysisResponse",
    "RemediationResponse",
    # Agent utilities
    "create_agent_context",
    "validate_agent_request",
    "validate_agent_response",
    # LLM types
    "ProviderT",
    "ModelT",
    "LLMResponseT",
    "ProviderType",
    "ProviderStatus",
    "ModelType",
    "ModelStatus",
    "ModelCapability",
    "RequestRole",
    "ResponseRole",
    "LLMContentType",
    "ContentFormat",
    "TokenCount",
    "TokenUsage",
    "CostPerToken",
    "TotalCost",
    "CostBreakdown",
    "Latency",
    "Throughput",
    "RateLimit",
    "ErrorType",
    "LLMErrorCode",
    "RetryAfter",
    "ModelConfig",
    "ProviderConfig",
    "LLMConfig",
    # LLM protocols
    "LLMProvider",
    "LLMModel",
    "LLMRequest",
    "LLMResponse",
    "Message",
    "StreamingResponse",
    "StreamingChunk",
    "ChatProvider",
    "CompletionProvider",
    "EmbeddingProvider",
    "ChatRequest",
    "ChatResponse",
    "CompletionRequest",
    "CompletionResponse",
    "EmbeddingRequest",
    "EmbeddingResponse",
    # LLM utilities
    "create_message",
    "calculate_token_cost",
    "create_usage_dict",
    "validate_model_config",
]
